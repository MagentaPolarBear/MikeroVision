"""
This script performs an image analysis on a set of microscopy images to identify, measure, and visualize particles. 
The main steps include:
1. Reading and cropping the images.
2. Enhancing the contrast using histogram equalization.
3. Applying a Gaussian filter to smooth the images.
4. Converting the images to binary using Otsu's thresholding method.
5. Applying morphological closing to the binary image.
6. Applying the distance transform and smoothing it.
7. Performing watershed segmentation to separate touching particles.
8. Measuring the major axis lengths of non-edge touching particles.
9. Drawing arrows to visualize the major axes and displaying individual histograms for each image.


The result includes intermediate visualizations for each image and a combined histogram showing the distribution of major axis lengths across all images.

Special thanks to Maaike Sikkink who helped lay the theoretical foundations 

Author: Mike Essink
Copyright (c) 2023, Mike Essink
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
    his list of conditions, and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
    this list of conditions, and the following disclaimer in the documentation
    and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE 
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE 
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

import numpy as np
from scipy.spatial.distance import pdist, squareform
from skimage.filters import threshold_otsu, gaussian
from skimage.morphology import binary_opening
from skimage.measure import find_contours
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from skimage.color import label2rgb
from skimage import exposure
from scipy import ndimage
import matplotlib.pyplot as plt
from PIL import Image
import seaborn as sns
from scipy.stats import norm
from scipy.spatial.distance import pdist, squareform
from skimage.filters import threshold_otsu, gaussian
from skimage.morphology import binary_opening, binary_closing

# Function to draw an arrow between two points
def draw_arrow(ax, start_point, end_point, distance_value):
    arrowprops = dict(facecolor='red', edgecolor='red', arrowstyle='->')
    ax.annotate("", xy=end_point, xytext=start_point, arrowprops=arrowprops)
    ax.text((start_point[0] + end_point[0]) / 2, (start_point[1] + end_point[1]) / 2, f"{distance_value:.2f}", color='red')

# List of file paths and corresponding magnifications
file_info = [
    # ('file_path', 'magnification')
   ('D:\\Microscopy Images\\PolyStyrene\\5x_1.TIF','5x'),
    # ... insert more paths 
   ('D:\\Microscopy Images\\PolyStyrene\\10x_1.TIF','10x'),
    # ... insert more paths 
   ('D:\\Microscopy Images\\PolyStyrene\\20x_1.TIF','20x'),
    # ... insert more paths 
   ('D:\\Microscopy Images\\PolyStyrene\\40x_1.TIF','40x'),
    # ... insert more paths 
]

# Scaling factors
scaling_factors = {'5x': 1.2816, '10x': 1.2816/2, '20x': 1.2816/4, '40x': 1.2816/8,}

# Accumulator for all major axis lengths
all_major_axis_lengths = []

for i, (file_path, magnification) in enumerate(file_info):
    print(f"Processing file {i + 1}/{len(file_info)}: {file_path}")

    scaling_factor = scaling_factors[magnification]

    # Read the image as a numpy array
    image_sample = Image.open(file_path)
    image_array = np.array(image_sample)
    cropped_image = image_array[80:-80, 80:-80]

    # Enhance contrast based on magnification
    if magnification in ['5x', '10x']:
        bright_image = exposure.equalize_adapthist(cropped_image) # or exposure.equalize_hist depending on lighting and the use of a polarizer/filter
    elif magnification in ['20x', '40x']:
        bright_image = exposure.equalize_adapthist(cropped_image)  # CLAHE method

    # Apply Gaussian filter based on magnification
    sigma_value = {'5x': 7, '10x': 7, '20x': 2, '40x': 2}[magnification]  # adjust values for gaussian operations
    filtered_image = gaussian(bright_image, sigma=sigma_value)

    # Thresholding using Otsu's method
    level = threshold_otsu(filtered_image)
    binary_image = filtered_image > level

    # Apply morphological closing to the binary image
    closed_binary_image = binary_closing(binary_image)
    
    # Compute the distance transform of the closed binary image
    distance = ndimage.distance_transform_edt(closed_binary_image)

    # Remove small objects
    # inverted_binary_image = binary_opening(inverted_binary_image) #activate if needed

    # Compute the distance transform of the binary image
    distance = ndimage.distance_transform_edt(closed_binary_image)

    # Smooth the distance transform
    smooth_distance = gaussian(distance, sigma=sigma_value)

    # Watershed parameters based on magnification
    footprint_size = {'5x': (1000,1000), '10x': (300, 300), '20x': (100, 100), '40x': (100, 100)}[magnification]  # adjust values for watershed
    local_maxi = peak_local_max(smooth_distance, footprint=np.ones(footprint_size), labels=closed_binary_image)

    # Create markers
    markers = np.zeros_like(distance, dtype=int)
    markers[local_maxi[:, 0], local_maxi[:, 1]] = np.arange(len(local_maxi)) + 1

    # Apply the watershed algorithm using the smoothed distance transform
    watershed_labels = watershed(-smooth_distance, markers, mask=closed_binary_image)

    # Overlaying the labeled image on the filtered image
    label_overlay = label2rgb(watershed_labels, image=filtered_image, bg_label=0)

    # Plot intermediate results
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes[0, 0].imshow(cropped_image, cmap='gray')
    axes[0, 0].set_title('Original Image')
    axes[0, 1].imshow(filtered_image, cmap='gray')
    axes[0, 1].set_title('Filtered Image')
    axes[0, 2].imshow(binary_image, cmap='gray')
    axes[0, 2].set_title('Binary Image')
    axes[1, 0].imshow(closed_binary_image, cmap='gray')
    axes[1, 0].set_title('Closed image')
    axes[1, 1].imshow(label_overlay)
    axes[1, 1].set_title('Watershed Segmentation')

    # Major axis lengths excluding edge-touching particles
    major_axis_lengths_excluded = []

    # Process each particle, excluding those touching the edges
    for region_label in np.unique(watershed_labels):
        if region_label == 0:
            continue
        particle = (watershed_labels == region_label)

        # Check if the particle touches the edge
        if np.any(particle[0, :]) or np.any(particle[-1, :]) or np.any(particle[:, 0]) or np.any(particle[:, -1]):
            continue

        # Find contours of the particle
        contours = find_contours(particle, 0.5)
        boundary = contours[0]

        # Calculate distances between all points on the boundary
        dists = pdist(boundary, 'euclidean')
        dists_matrix = squareform(dists)

        # Find the maximum distance (which will correspond to the major axis length)
        max_dist, idx = np.max(dists_matrix), np.argmax(dists_matrix)
        scaled_dist = max_dist * scaling_factor

        # Retrieve the start and end points of the major axis
        start_idx, end_idx = np.unravel_index(idx, dists_matrix.shape)
        start_point = boundary[start_idx]
        end_point = boundary[end_idx]

        # Draw an arrow between the start and end points with distance value
        draw_arrow(axes[1, 1], start_point[::-1], end_point[::-1], scaled_dist)

        # Store major axis length
        major_axis_lengths_excluded.append(scaled_dist)

    # Individual histogram for the current image
    axes[1, 2].hist(major_axis_lengths_excluded, bins=20, edgecolor='black')
    axes[1, 2].set_title('Individual Histogram')
    axes[1, 2].set_xlabel('Major Axis Length (um)')
    axes[1, 2].set_ylabel('Frequency')

    # Add to the accumulator
    all_major_axis_lengths.extend(major_axis_lengths_excluded)

    # Save or display the figure (optional)
    plt.savefig(f'intermediate_figure_{i}.png')
    #plt.show()                 #take away the # and you obtain the plots as popup one by one


# Plot a combined histogram for all images
plt.figure(figsize=(10, 6))
plt.hist(all_major_axis_lengths, bins=100, edgecolor='black')
plt.xlabel('Major Axis Length (um)')
plt.ylabel('Frequency')
plt.title('Combined Major Axis Length Distribution')
plt.grid(True)
plt.show()

###########################################################################################
########################################################################################
#   code to plot the probability density function
plt.figure(figsize=(12, 6))
sns.kdeplot(all_major_axis_lengths, fill=True)
plt.title('Probability Density Function')
plt.xlabel('Major Axis Length (um)')
plt.ylabel('Density')
plt.grid(True)
plt.xlim([0, None])
plt.show()

# Calculating error margins (95% confidence interval)
mean_length = np.mean(all_major_axis_lengths)
standard_error = np.std(all_major_axis_lengths) / np.sqrt(len(all_major_axis_lengths))
margin_of_error = 1.96 * standard_error  # 95% confidence interval
lower_bound = mean_length - margin_of_error
upper_bound = mean_length + margin_of_error

print(f"Mean Length: {mean_length:.2f} um")
print(f"95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f}) um")

# Calculating and plotting percentiles
lower_25_percentile = np.percentile(all_major_axis_lengths, 25)
middle_50_percentile = np.percentile(all_major_axis_lengths, 50)
upper_25_percentile = np.percentile(all_major_axis_lengths, 75)

plt.figure(figsize=(12, 6))
plt.hist(all_major_axis_lengths, bins=100, edgecolor='black', alpha=0.5)
plt.axvline(lower_25_percentile, color='red', linestyle='--', label='25th Percentile')
plt.axvline(middle_50_percentile, color='green', linestyle='--', label='50th Percentile')
plt.axvline(upper_25_percentile, color='blue', linestyle='--', label='75th Percentile')
plt.xlabel('Major Axis Length (um)')
plt.ylabel('Frequency')
plt.title('Distribution with Percentiles')
plt.legend()
plt.grid(True)
plt.show()

print(f"Lower 25% Level: {lower_25_percentile:.2f} um")
print(f"Middle 50% Level: {middle_50_percentile:.2f} um")
print(f"Upper 25% Level: {upper_25_percentile:.2f} um")

print("finished")
